{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make basic plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "#import coffea.nanoevents.methods\n",
    "import coffea.processor as processor\n",
    "import awkward1 as ak\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys, time, uproot_methods.classes.TVector3\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add coffea packages to the import path\n",
    "venv_path=os.environ['VIRTUAL_ENV']\n",
    "site_path=venv_path+\"/lib/python3.6/site-packages/\"\n",
    "sys.path.insert(0, site_path)\n",
    "\n",
    "# Add local packages to the import path\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# XRootD settings\n",
    "xrootd_endpoint=\"root://hepxrd01.colorado.edu:1094/\"\n",
    "xrootd_base_path=\"/store/user/aperloff/ExoEMJAnalysis2020/\"\n",
    "\n",
    "# Dask settings\n",
    "useDask = False\n",
    "if useDask:\n",
    "    from distributed import Client\n",
    "    client = Client('coffea-dask.fnal.gov:8786')\n",
    "\n",
    "# Print settings\n",
    "verbose = False\n",
    "\n",
    "# Debug settings\n",
    "debug = True\n",
    "if debug:\n",
    "    verbose = True\n",
    "    xrootd_base_path=\"/store/user/aperloff/ExoEMJAnalysis2020/Run2ProductionV18eDebug/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first and last (reload) import lines allow for making changes\n",
    "#  in the xrootd module and reloading it in the same session.\n",
    "import utils.python.xrootd\n",
    "from utils.python.xrootd import *\n",
    "importlib.reload(utils.python.xrootd)\n",
    "\n",
    "condorSub_dicts = [\"EMJ2016\",\"EMJ2017\",\"EMJ2018\",\n",
    "                   \"Summer16v3_qcd\",\"Fall17_qcd\",\"Autumn18_qcd\",\n",
    "                   \"Summer16v3_gjets\",\"Fall17_gjets\",\"Autumn18_gjets\"]\n",
    "sample_dict = get_files_xrootd(xrootd_endpoint,\n",
    "                               xrootd_base_path,\n",
    "                               dicts=condorSub_dicts,\n",
    "                               verbose=verbose,\n",
    "                               debug=((3,2) if debug else None)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMJProcessor(processor.ProcessorABC):\n",
    "\n",
    "    class Charge(Enum):\n",
    "        FAILED = -1\n",
    "        NEUTRAL = 0\n",
    "        CHARGED = 1\n",
    "    \n",
    "    def __init__(self):\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"Primary dataset\")\n",
    "        pt_axis = hist.Bin(\"pt\", r\"$p_{T}$ [GeV]\", \n",
    "                           np.array([0,5,10,15,20,25,30,35,40,45,50,60,70,80,90,\n",
    "                                     100,120,140,160,180,\n",
    "                                     200,250,300,350,400,450,500,\n",
    "                                     600,700,800,900,1000,\n",
    "                                     1500,2000,3000,4000,5000]))\n",
    "        eta_axis = hist.Bin(\"eta\", r\"$\\eta$\", 20, -5, 5)\n",
    "        phi_axis = hist.Bin(\"phi\", r\"$\\phi$\",64, -3.2, 3.2)\n",
    "        e_axis = hist.Bin(\"e\", r\"energy$ [GeV]\", 200, 0, 500)\n",
    "        chi2_axis = hist.Bin(\"chi2\",r\"$\\chi^{2}/NDF$\", 20, 0, 20)\n",
    "        ip2d_axis = hist.Bin(\"ip2d\",r\"IP_{2D}\", 100, 0, 1)\n",
    "        ip2d_sig_axis = hist.Bin(\"ip2d_sig\",r\"IP_{2D,sig}\", 100, 0, 5)\n",
    "        ipz_axis = hist.Bin(\"ipz\",r\"IP_{z}\", 100, 0, 10)\n",
    "        nhits_axis = hist.Bin(\"nhits\",r\"N_{hits}\", 40, 0, 40)\n",
    "        npix_axis = hist.Bin(\"npix\",r\"N_{pix}\", 10, 0, 10)\n",
    "        tknum_axis = hist.Bin(\"tknum\", r\"N_{track}\", 50, 0, 500)\n",
    "        partnum_axis = hist.Bin(\"partnum\", r\"N_{particles}\", 500, 0, 5000)\n",
    "        pdgid_axis = hist.Bin(\"pdgid\", r\"PDGID\", 5000, 0, 5000)\n",
    "        x_axis = hist.Bin(\"x\", r\"X\", 600, -30, 30)\n",
    "        y_axis = hist.Bin(\"y\", r\"Y\", 600, -30, 30)\n",
    "        z_axis = hist.Bin(\"z\", r\"Z\", 600, -30, 30)\n",
    "        rho_axis = hist.Bin(\"rho\", r\"\\rho\", 600, 0, 30)\n",
    "        mult_axis = hist.Bin(\"mult\", r\"Multiplicity\", 70, 0, 70)\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            # Final state particle counting Particle counting\n",
    "            'NumFinal' : hist.Hist(\"Counts\", dataset_axis, partnum_axis),\n",
    "            'NumChargeFinal' : hist.Hist(\"Counts\", dataset_axis, partnum_axis),\n",
    "            'FinalPDGID' : hist.Hist(\"Counts\", dataset_axis, pdgid_axis),\n",
    "            # Unlike reco::GenParticles used in https://gitlab.cern.ch/yichen/emj-analyze/-/blob/master/plugins/GenHistogram.cc\n",
    "            #  we don't store the vertex position associated to the GenParticle\n",
    "            #'FinalGenX' : hist.Hist(\"Counts\", dataset_axis, x_axis),\n",
    "            #'FinalGenY' : hist.Hist(\"Counts\", dataset_axis, y_axis),\n",
    "            #'FinalGenZ' : hist.Hist(\"Counts\", dataset_axis, z_axis),\n",
    "            'FinalPt' : hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'FinalEta' : hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            'FinalPhi' : hist.Hist(\"Counts\", dataset_axis, phi_axis),\n",
    "            'FinalE' : hist.Hist(\"Counts\", dataset_axis, e_axis),\n",
    "\n",
    "            # Hidden valley particles from decay chain\n",
    "            \"DecaySummary\" : hist.Hist(\"Counts\", dataset_axis, mult_axis),\n",
    "            \"DecayDQuarkMult\" : hist.Hist(\"Counts\", dataset_axis, mult_axis),\n",
    "            \"DecayDMesonMult_Imm\" : hist.Hist(\"Counts\", dataset_axis, mult_axis),\n",
    "            \"DecayDMesonMult_All\" : hist.Hist(\"Counts\", dataset_axis, mult_axis),\n",
    "            \"DarkDecayGenRho\" : hist.Hist(\"Counts\", dataset_axis, rho_axis),\n",
    "            \"DarkDecayGenEta\": hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            \"DarkDecayGenZ\" : hist.Hist(\"Counts\", dataset_axis, z_axis),\n",
    "            \n",
    "            # Reco track plots\n",
    "            'tkpt' : hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'tketa' : hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            'tkphi' : hist.Hist(\"Counts\", dataset_axis, phi_axis),\n",
    "            'tknum' : hist.Hist(\"Counts\", dataset_axis, tknum_axis),\n",
    "            'tkchi2' : hist.Hist(\"Counts\", dataset_axis, chi2_axis),\n",
    "            'tkip2d' : hist.Hist(\"Counts\", dataset_axis, ip2d_axis),\n",
    "            'tkip2d_sig' : hist.Hist(\"Counts\", dataset_axis, ip2d_sig_axis),\n",
    "            'tkipz' : hist.Hist(\"Counts\", dataset_axis, ipz_axis),\n",
    "            'tknhits' : hist.Hist(\"Counts\", dataset_axis, nhits_axis),\n",
    "            'tknpix' : hist.Hist(\"Counts\", dataset_axis, npix_axis),\n",
    "\n",
    "            # Reco jet plots\n",
    "            'jtpt':hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'jteta':hist.Hist(\"Counts\", dataset_axis, eta_axis),\n",
    "            'jtphi':hist.Hist(\"Counts\", dataset_axis, phi_axis),\n",
    "            'jte':hist.Hist(\"Counts\", dataset_axis, e_axis),\n",
    "            \n",
    "            # Simple accumulators\n",
    "            'cutflow': processor.defaultdict_accumulator(int),\n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "\n",
    "        dataset = df['dataset']\n",
    "        \n",
    "        Jets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['Jets'],\n",
    "            pt=df['Jets.fCoordinates.fPt'],\n",
    "            eta=df['Jets.fCoordinates.fEta'],\n",
    "            phi=df['Jets.fCoordinates.fPhi'],\n",
    "            energy=df['Jets.fCoordinates.fE'],\n",
    "            axismajor=df['Jets_axismajor'],\n",
    "            axisminor=df['Jets_axisminor'],\n",
    "            bDiscriminatorCSV=df['Jets_bDiscriminatorCSV'],\n",
    "            bJetTagDeepCSVBvsAll=df['Jets_bJetTagDeepCSVBvsAll'],\n",
    "            chargedEmEnergyFraction=df['Jets_chargedEmEnergyFraction'],\n",
    "            chargedHadronEnergyFraction=df['Jets_chargedHadronEnergyFraction'],\n",
    "            chargedHadronMultiplicity=df['Jets_chargedHadronMultiplicity'],\n",
    "            chargedMultiplicity=df['Jets_chargedMultiplicity'],\n",
    "            electronEnergyFraction=df['Jets_electronEnergyFraction'],\n",
    "            electronMultiplicity=df['Jets_electronMultiplicity'],\n",
    "            hadronFlavor=df['Jets_hadronFlavor'],\n",
    "            hfEMEnergyFraction=df['Jets_hfEMEnergyFraction'],\n",
    "            hfHadronEnergyFraction=df['Jets_hfHadronEnergyFraction'],\n",
    "            HTMask=df['Jets_HTMask'],\n",
    "            ID=df['Jets_ID'],\n",
    "            jecFactor=df['Jets_jecFactor'],\n",
    "            jecUnc=df['Jets_jecUnc'],\n",
    "            jerFactor=df['Jets_jerFactor'],\n",
    "            jerFactorDown=df['Jets_jerFactorDown'],\n",
    "            jerFactorUp=df['Jets_jerFactorUp'],\n",
    "            LeptonMask=df['Jets_LeptonMask'],\n",
    "            MHTMask=df['Jets_MHTMask'],\n",
    "            multiplicity=df['Jets_multiplicity'],\n",
    "            muonEnergyFraction=df['Jets_muonEnergyFraction'],\n",
    "            muonMultiplicity=df['Jets_muonMultiplicity'],\n",
    "            neutralEmEnergyFraction=df['Jets_neutralEmEnergyFraction'],\n",
    "            neutralHadronEnergyFraction=df['Jets_neutralHadronEnergyFraction'],\n",
    "            neutralHadronMultiplicity=df['Jets_neutralHadronMultiplicity'],\n",
    "            neutralMultiplicity=df['Jets_neutralMultiplicity'],\n",
    "            origIndex=df['Jets_origIndex'],\n",
    "            partonFlavor=df['Jets_partonFlavor'],\n",
    "            photonEnergyFraction=df['Jets_photonEnergyFraction'],\n",
    "            photonMultiplicity=df['Jets_photonMultiplicity'],\n",
    "            ptD=df['Jets_ptD'],\n",
    "            qgLikelihood=df['Jets_qgLikelihood'],\n",
    "        )    \n",
    "        \n",
    "        GenJets = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['GenJets'],\n",
    "            pt=df['GenJets.fCoordinates.fPt'],\n",
    "            eta=df['GenJets.fCoordinates.fEta'],\n",
    "            phi=df['GenJets.fCoordinates.fPhi'],\n",
    "            energy=df['GenJets.fCoordinates.fE'],\n",
    "        )\n",
    "\n",
    "        GenParticles = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['GenParticles'],\n",
    "            pt=df['GenParticles.fCoordinates.fPt'],\n",
    "            eta=df['GenParticles.fCoordinates.fEta'],\n",
    "            phi=df['GenParticles.fCoordinates.fPhi'],\n",
    "            energy=df['GenParticles.fCoordinates.fE'],\n",
    "            #Charge=df['GenParticles.Charge'],\n",
    "            ParentId=df['GenParticles_ParentId'],\n",
    "            ParentIdx=df['GenParticles_ParentIdx'],\n",
    "            PdgId=df['GenParticles_PdgId'],\n",
    "            Status=df['GenParticles_Status'],\n",
    "        )\n",
    "\n",
    "        PrimaryVertices = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['PrimaryVertices_position.fCoordinates.fX'],\n",
    "            px=df['PrimaryVertices_position.fCoordinates.fX'],\n",
    "            py=df['PrimaryVertices_position.fCoordinates.fY'],\n",
    "            pz=df['PrimaryVertices_position.fCoordinates.fZ'],\n",
    "            mass=np.zeros_like(df['PrimaryVertices_position.fCoordinates.fX']),\n",
    "            chi2=df['PrimaryVertices_chi2'],\n",
    "            isFake=df['PrimaryVertices_isFake'],\n",
    "            isGood=df['PrimaryVertices_isGood'],\n",
    "            isValid=df['PrimaryVertices_isValid'],\n",
    "            ndof=df['PrimaryVertices_ndof'],\n",
    "            nTracks=df['PrimaryVertices_nTracks'],\n",
    "            position=df['PrimaryVertices_position'],\n",
    "            tError=df['PrimaryVertices_tError'],\n",
    "            time=df['PrimaryVertices_time'],\n",
    "            xError=df['PrimaryVertices_xError'],\n",
    "            yError=df['PrimaryVertices_yError'],\n",
    "            zError=df['PrimaryVertices_zError'],\n",
    "        )\n",
    "        \n",
    "        #Access with Tracks.p4.{x,y,z,mass/energy=t}\n",
    "        Tracks = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['Tracks'],\n",
    "            px=df['Tracks.fCoordinates.fX'],\n",
    "            py=df['Tracks.fCoordinates.fY'],\n",
    "            pz=df['Tracks.fCoordinates.fZ'],\n",
    "            mass=np.zeros_like(df['Tracks.fCoordinates.fX']),\n",
    "            charge=df['Tracks_charge'],\n",
    "            dxyErrorPV0=df['Tracks_dxyErrorPV0'],\n",
    "            dxyPV0=df['Tracks_dxyPV0'],\n",
    "            dzAssociatedPV=df['Tracks_dzAssociatedPV'],\n",
    "            dzErrorPV0=df['Tracks_dzErrorPV0'],\n",
    "            dzPV0=df['Tracks_dzPV0'],\n",
    "            etaError=df['Tracks_etaError'],\n",
    "            firstHit=df['Tracks_firstHit'],\n",
    "            foundHits=df['Tracks_foundHits'],\n",
    "            fromPV0=df['Tracks_fromPV0'],\n",
    "            hitPattern=df['Tracks_hitPattern'],\n",
    "            hitPatternOffsets=df['Tracks_hitPatternOffsets'],\n",
    "            IP2DPV0=df['Tracks_IP2DPV0'],\n",
    "            IP2dSigPV0=df['Tracks_IP2dSigPV0'],\n",
    "            IP3DPV0=df['Tracks_IP3DPV0'],\n",
    "            IP3DSigPV0=df['Tracks_IP3DSigPV0'],\n",
    "            IPzPV0 = np.sqrt(df['Tracks_IP3DPV0']**2 - df['Tracks_IP2DPV0']**2),\n",
    "            lostHits=df['Tracks_lostHits'],\n",
    "            matchedToPFCandidate=df['Tracks_matchedToPFCandidate'],\n",
    "            normalizedChi2=df['Tracks_normalizedChi2'],\n",
    "            numberOfHits=df['Tracks_numberOfHits'],\n",
    "            numberOfPixelHits=df['Tracks_numberOfPixelHits'],\n",
    "            phiError=df['Tracks_phiError'],\n",
    "            ptError=df['Tracks_ptError'],\n",
    "            pvAssociationQuality=df['Tracks_pvAssociationQuality'],\n",
    "            qoverpError=df['Tracks_qoverpError'],\n",
    "            quality=df['Tracks_quality'],\n",
    "        )\n",
    "        Tracks_referencePoint=ak.zip({\"x\": df['Tracks_referencePoint.fCoordinates.fX'],\n",
    "                                      \"y\": df['Tracks_referencePoint.fCoordinates.fY'],\n",
    "                                      \"z\": df['Tracks_referencePoint.fCoordinates.fZ']},\n",
    "                                     with_name=\"ThreeVector\")\n",
    "        \n",
    "        \n",
    "        evtweights = df[\"Weight\"].reshape(-1, 1).flatten()\n",
    "        output['cutflow']['all events'] += Jets.size\n",
    "\n",
    "        jetId_cut = (Jets.ID > 0)        \n",
    "        Jets = Jets[jetId_cut]\n",
    "        output['cutflow']['>=1 with loose id'] += jetId_cut.any().sum()\n",
    "        twoJets = (Jets.counts >= 2)        \n",
    "        output['cutflow']['>=2 reco jets'] += twoJets.sum()\n",
    "        twoGens = (GenJets.counts >= 2)\n",
    "        output['cutflow']['>=2 gen jets'] += twoGens.sum()\n",
    "        \n",
    "        #Jets = Jets[twoJets & twoGens]\n",
    "        #GenJets = GenJets[twoJets & twoGens]\n",
    "        \n",
    "        \n",
    "        #dphi_index = Jets.p4[:,0].delta_phi( Jets.p4[:,1] ) > 1.8\n",
    "        #output['cutflow']['dPhi > 1.8'] += dphi_index.sum()\n",
    "        \n",
    "\n",
    "        #Jets = Jets[dphi_index]\n",
    "        #GenJets = GenJets[dphi_index]\n",
    "        \n",
    "        #pairing = Jets.p4[:,0:2].cross(GenJets.p4, nested=True)\n",
    "        #metric = pairing.i0.delta_r(pairing.i1)\n",
    "        \n",
    "        #index_of_minimized = metric.argmin()\n",
    "        #dr_cut = (metric[index_of_minimized] < 0.2)\n",
    "        #best_pairings_that_pass_dr_cut = pairing[index_of_minimized][dr_cut]\n",
    "        #genrecos = best_pairings_that_pass_dr_cut.flatten(axis=1)\n",
    "        #ptresponse = genrecos.i0.pt / genrecos.i1.pt\n",
    "        \n",
    "        #\n",
    "        # Calculate non-trivial GEN quantities\n",
    "        #\n",
    "        \n",
    "        # Mask and collection for only status==1 GenParticles\n",
    "        genParticles_status1_mask = (GenParticles.Status == 1)\n",
    "        genParticles_status1 = GenParticles[genParticles_status1_mask]\n",
    "        \n",
    "        # Mask and collection for the charged particles\n",
    "        # Currently the charge isn't stored.\n",
    "        #genParticles_charged_mask = (GenParticles.Charge != 0)\n",
    "        # This is a temporary pass-through\n",
    "        genParticles_charged_mask = (GenParticles.Status)\n",
    "        \n",
    "        # Calculate the combined filter and collection\n",
    "        genParticles_status1_charged_mask = (genParticles_status1_mask & genParticles_charged_mask)\n",
    "        num_final         = genParticles_status1_charged_mask.sum()\n",
    "        num_final_charged = 0\n",
    "\n",
    "        #\n",
    "        # Fill the GEN histograms\n",
    "        #\n",
    "        output['FinalPDGID'].fill(dataset=dataset, pdgid=genParticles_status1.PdgId.flatten())\n",
    "        output['FinalPt'].fill(dataset=dataset, pt=genParticles_status1.pt.flatten())\n",
    "        output['FinalEta'].fill(dataset=dataset, eta=genParticles_status1.eta.flatten())\n",
    "        output['FinalPhi'].fill(dataset=dataset, phi=genParticles_status1.phi.flatten())\n",
    "        output['FinalE'].fill(dataset=dataset, e=genParticles_status1.p4.energy.flatten())\n",
    "        \n",
    "        #\n",
    "        # Fill the RECO histograms\n",
    "        #\n",
    "        output['jtpt'].fill(dataset=dataset, pt=Jets.pt.flatten())\n",
    "        output['jteta'].fill(dataset=dataset, eta=Jets.eta.flatten())\n",
    "        output['jtphi'].fill(dataset=dataset, phi=Jets.phi.flatten())\n",
    "        output['jte'].fill(dataset=dataset, e=Jets.p4.energy.flatten())\n",
    "        output['tkpt'].fill(dataset=dataset, pt=Tracks.pt.flatten())\n",
    "        output['tketa'].fill(dataset=dataset, eta=Tracks.eta.flatten())\n",
    "        output['tkphi'].fill(dataset=dataset, phi=Tracks.phi.flatten())\n",
    "        output['tknum'].fill(dataset=dataset, tknum=Tracks.counts)\n",
    "        output['tkchi2'].fill(dataset=dataset, chi2=Tracks.normalizedChi2.flatten())\n",
    "        output['tkip2d'].fill(dataset=dataset, ip2d=Tracks.IP2DPV0.flatten())\n",
    "        output['tkip2d_sig'].fill(dataset=dataset, ip2d_sig=Tracks.IP2dSigPV0.flatten())\n",
    "        output['tkipz'].fill(dataset=dataset, ipz=Tracks.IPzPV0.flatten())\n",
    "        output['tknhits'].fill(dataset=dataset, nhits=Tracks.numberOfHits.flatten())\n",
    "        output['tknpix'].fill(dataset=dataset, npix=Tracks.numberOfPixelHits.flatten())\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstart = time.time() \n",
    "\n",
    "if not useDask:\n",
    "    output = processor.run_uproot_job(sample_dict,\n",
    "                                      treename='TreeMaker2/PreSelection',\n",
    "                                      processor_instance=EMJProcessor(),\n",
    "                                      executor=processor.iterative_executor,\n",
    "                                      executor_args={\n",
    "                                          'skipbadfiles':True,\n",
    "                                          'nano':False, \n",
    "                                          'flatten':True, \n",
    "                                          'workers': 4},\n",
    "                                      chunksize=50000, maxchunks=100\n",
    "                                     )\n",
    "else:\n",
    "    output = processor.run_uproot_job(sample_dict,\n",
    "                                      treename='TreeMaker2/PreSelection',\n",
    "                                      processor_instance=EMJProcessor(),\n",
    "                                      executor=processor.dask_executor,\n",
    "                                      executor_args={\n",
    "                                          'skipbadfiles':True,\n",
    "                                          'client': client, \n",
    "                                          'nano':False, \n",
    "                                          'flatten':True, \n",
    "                                          'workers': 2},\n",
    "                                      chunksize=50000\n",
    "                                     )\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Events/s:\", output['cutflow']['all events']/elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything contained in the 'output' dictionary\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cutflow accumulator\n",
    "output['cutflow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all of the processes on the 'dataset' axis for the jtpt histogram\n",
    "output['jtpt'].axis('dataset').identifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the values in one of the histograms\n",
    "output['jtpt']['EMJ_2016_mMed-1000_mDark-20_kappa-0p12_aligned-down'].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.python.accumulator import *\n",
    "if debug:\n",
    "    test_filter_histograms(output)\n",
    "    print('\\n\\n')\n",
    "    test_integrate_hist_over_dataset(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"image.cmap\"] = 'Blues'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jet Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hnames = ['jtpt','jteta','jtphi','jte']\n",
    "logx = [1,0,0,1]\n",
    "logy = [1,0,0,1]\n",
    "for ih, hname in enumerate(hnames):\n",
    "    ax = hist.plotgrid(output[hname], overlay=\"dataset\", stack=False, density=True,\n",
    "                       #fill_opts=stack_fill_opts,\n",
    "                       #error_opts=stack_error_opts,\n",
    "                      )\n",
    "    if logx[ih]: plt.xscale(\"log\")\n",
    "    if logy[ih]: plt.yscale(\"log\")\n",
    "    for iax in ax.flatten():\n",
    "        iax.autoscale(axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hnames = ['tkpt','tketa','tkphi','tknum','tkchi2','tkip2d','tkip2d_sig','tkipz','tknhits','tknpix']\n",
    "logx = [1,0,0,0,0,0,0,0,0,0]\n",
    "logy = [1,0,0,0,1,1,0,1,0,0]\n",
    "for ih, hname in enumerate(hnames):\n",
    "    ax = hist.plotgrid(output[hname], overlay=\"dataset\", stack=False, density=True,\n",
    "                       #fill_opts=stack_fill_opts,\n",
    "                       #error_opts=stack_error_opts,\n",
    "                      )\n",
    "    if logx[ih]: plt.xscale(\"log\")\n",
    "    if logy[ih]: plt.yscale(\"log\")\n",
    "    for iax in ax.flatten():\n",
    "        iax.autoscale(axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffeaenv",
   "language": "python",
   "name": "coffeaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
